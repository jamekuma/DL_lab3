<div class=WordSection1 style='layout-grid:15.6pt'>


<p class=MsoNormal align=center style='text-align:center'><a name="OLE_LINK2"></a><a
name="OLE_LINK1"><span style='mso-bookmark:OLE_LINK2'><span style='font-size:
36.0pt;font-family:华文隶书'>哈尔滨工业大学<span lang=EN-US><o:p></o:p></span></span></span></a></p>



<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span lang=EN-US style='font-size:26.0pt;
mso-fareast-font-family:黑体'>&lt;&lt;</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>模式识别与深度学习</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'>&gt;&gt;<o:p></o:p></span></b></span></span></p>
<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span style='font-size:26.0pt;font-family:
黑体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri'>实验</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'>3 </span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>实验报告</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'><o:p></o:p></span></b></span></span></p>
<p class=MsoNormal align=center style='text-align:center;line-height:50.0pt;
mso-line-height-rule:exactly'><span style='mso-bookmark:OLE_LINK1'><span
style='mso-bookmark:OLE_LINK2'><b><span style='font-size:26.0pt;font-family:
黑体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri'>（2020</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'></span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
style='font-size:26.0pt;font-family:黑体;mso-ascii-font-family:Calibri;
mso-hansi-font-family:Calibri'>春季学期）</span></b></span></span><span
style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b><span
lang=EN-US style='font-size:26.0pt;mso-fareast-font-family:黑体'><o:p></o:p></span></b></span></span></p>






<div align=center>
<table class=MsoNormalTable border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none;mso-border-top-alt:1.5pt;
 mso-border-left-alt:.5pt;mso-border-bottom-alt:1.5pt;mso-border-right-alt:
 .5pt;mso-border-color-alt:windowtext;mso-border-style-alt:solid;mso-yfti-tbllook:
 480;mso-padding-alt:0cm 5.4pt 0cm 5.4pt;mso-border-insideh:.5pt solid windowtext;
 mso-border-insidev:.5pt solid windowtext'>
 <tr style='mso-yfti-irow:0;mso-yfti-firstrow:yes'>
  <td width=189 style='width:141.5pt;border:solid windowtext 1.0pt;border-top:
  solid windowtext 1.5pt;mso-border-alt:solid windowtext .5pt;mso-border-top-alt:
  solid windowtext 1.5pt;background:#CCECFF;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 align=right style='text-align:right;text-indent:32.15pt'><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span style='font-size:16.0pt;font-family:
  楷体_GB2312;mso-hansi-font-family:"Times New Roman"'>成员<span lang=EN-US
  style='color:black;mso-color-alt:windowtext'>1</span><span style='color:black;
  mso-color-alt:windowtext'>：</span><span lang=EN-US><o:p></o:p></span></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
  <td width=239 style='width:178.9pt;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  mso-border-left-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;
  mso-border-top-alt:solid windowtext 1.5pt;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 style='text-align:justify;text-justify:inter-ideograph;
  text-indent:32.15pt'><span style='mso-bookmark:OLE_LINK1'><span
  style='mso-bookmark:OLE_LINK2'><b style='mso-bidi-font-weight:normal'><span
  lang=EN-US style='font-size:16.0pt;font-family:"Times New Roman",serif;
  mso-fareast-font-family:楷体_GB2312'>1170500913 </span></b></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><span
  class=GramE><b style='mso-bidi-font-weight:normal'><span style='font-size:
  16.0pt;font-family:楷体_GB2312;mso-ascii-font-family:"Times New Roman";
  mso-hansi-font-family:"Times New Roman"'>熊健羽</span></b></span></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:16.0pt;
  font-family:"Times New Roman",serif;mso-fareast-font-family:楷体_GB2312'><o:p></o:p></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
 </tr>
 <tr style='mso-yfti-irow:1'>
  <td width=189 style='width:141.5pt;border:solid windowtext 1.0pt;border-top:
  solid windowtext 1.5pt;mso-border-alt:solid windowtext .5pt;mso-border-top-alt:
  solid windowtext 1.5pt;background:#CCECFF;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 align=right style='text-align:right;text-indent:32.15pt'><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span style='font-size:16.0pt;font-family:
  楷体_GB2312;mso-hansi-font-family:"Times New Roman"'>成员<span lang=EN-US
  style='color:black;mso-color-alt:windowtext'>2</span><span style='color:black;
  mso-color-alt:windowtext'>：</span><span lang=EN-US><o:p></o:p></span></span></b></span></span></p>
  </td>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
  <td width=239 style='width:178.9pt;border-top:solid windowtext 1.5pt;
  border-left:none;border-bottom:solid windowtext 1.0pt;border-right:solid windowtext 1.0pt;
  mso-border-left-alt:solid windowtext .5pt;mso-border-alt:solid windowtext .5pt;
  mso-border-top-alt:solid windowtext 1.5pt;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=a3 style='text-align:justify;text-justify:inter-ideograph;
  text-indent:32.15pt'><span style='mso-bookmark:OLE_LINK1'><span
  style='mso-bookmark:OLE_LINK2'><b style='mso-bidi-font-weight:normal'><span
  lang=EN-US style='font-size:16.0pt;font-family:"Times New Roman",serif;
  mso-fareast-font-family:楷体_GB2312'>1171000520 </span></b></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><span
  class=GramE><b style='mso-bidi-font-weight:normal'><span style='font-size:
  16.0pt;font-family:楷体_GB2312;mso-ascii-font-family:"Times New Roman";
  mso-hansi-font-family:"Times New Roman"'>鲍克勤</span></b></span></span></span><span
  style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'><b
  style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:16.0pt;
  font-family:"Times New Roman",serif;mso-fareast-font-family:楷体_GB2312'><o:p></o:p>
  <span style='mso-bookmark:OLE_LINK1'><span style='mso-bookmark:OLE_LINK2'></span></span>
 </tr>



































## 实验目标

- 基于PyTorch实现VGG/ResNet/SENet结构

  - VGG要求Conv部分参照论文，可动态调整结构，测试结果不低于80%；

    ResNet要求基于残差块，参照论文，可动态调整，测试结果不低于80%；

    分别在上述VGG和ResNet基础上，添加SE block。

  - 基于VGG进行训练方式对比（学习率、优化器(SGD与Adam)对比）

  - 基于VGG进行data augmentation对比分析（基于上述最佳模型，讨论分析翻转、旋转、移位等操作对结果影响）

- 在Cifar-10数据集上进行验证

## 成员分工

1170500913 熊健羽：

- 构建、训练、测试VGG、VGG+SE block网络
- 进行VGG对比实验

1171000520 鲍克勤：

- 构建、训练、测试ResNet、ResNet+SE block网络
- 实现半精度

## 实验环境

实验环境为免费的Baidu AI studio。

操作系统：Ubuntu 16.04

CPU：8核

RAM：32G 

GPU：Tesla V100-SXM2

GPU RAM：16G

## 实验准备

### VGG

#### VGG原理

VGG16相比AlexNet的一个改进是**采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）**。对于给定的感受野，采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。

#### VGG结构

有多种常用的VGG，其本质区别在于卷积层的深度。按照网络的深度可分为VGG-11、VGG-13、VGG-16、VGG-19。具体结构如下表：

![img](https://pic4.zhimg.com/80/v2-ea924e733676e0da534f677a97c98653_1440w.jpg)



### ResNet

### SE block

#### SE block结构概述

SE block是2017年公布的Squeeze-and-Excitation Networks（SENet）网络中的一个特殊的结构。

![img](https://pic1.zhimg.com/80/v2-eb33a772a6029e5c8011a5ab77ea2f74_1440w.jpg)

网络的左半部分是一个传统的卷积变换。后半部分中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BU%7D) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=W%5Ctimes+H%5Ctimes+C) 的Feature Map， ![[公式]](https://www.zhihu.com/equation?tex=%28W%2CH%29) 是图像的尺寸， ![[公式]](https://www.zhihu.com/equation?tex=C) 是图像的通道数。

经过 ![[公式]](https://www.zhihu.com/equation?tex=F_%7Bsq%7D%28%5Ccdot%29) （Squeeze操作）后，图像变成了一个 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1%5Ctimes+C) 的特征向量，特征向量的值由 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BU%7D) 确定。经过 ![[公式]](https://www.zhihu.com/equation?tex=F_%7Bex%7D%28%5Ccdot%2C%5Cmathbf%7BW%7D%29) 后，特征向量的维度没有变，但是向量值变成了新的值。这些值会通过和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BU%7D) 的 ![[公式]](https://www.zhihu.com/equation?tex=F_%7Bscale%7D%28%5Ccdot%2C%5Ccdot%29) 得到加权后的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cmathbf%7BX%7D%7D) 。 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cmathbf%7BX%7D%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BU%7D) 的维度是相同的。

#### Squeeze

Squeeze部分的作用是获得Feature Map ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BU%7D) 的每个通道的全局信息嵌入（特征向量）。在SE block中，这一步通过VGG中引入的Global Average Pooling（GAP）实现的。也就是通过求每个通道 ![[公式]](https://www.zhihu.com/equation?tex=c%2C+c%5Cin%5C%7B1%2CC%5C%7D) 的Feature Map的平均值：

![[公式]](https://www.zhihu.com/equation?tex=z_c+%3D+%5Cmathbf%7BF%7D%7Bsq%7D%28%5Cmathbf%7Bu%7D_c%29+%3D+%5Cfrac%7B1%7D%7BW%5Ctimes+H%7D+%5Csum_%7Bi%3D1%7D%5EW%5Csum_%7Bj%3D1%7D%5EH+u_c%28i%2Cj%29++%5Ctag1)

#### Excitation

SE blocks使用了两层全连接构成的门机制（gate mechanism）。门控单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D) （即图中 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1%5Ctimes+C) 的特征向量）的计算法方式表示为：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cmathbf%7Bs%7D+%3D+%5Cmathbf%7BF%7D_%7Bex%7D%28%5Cmathbf%7Bz%7D%2C+%5Cmathbf%7BW%7D%29+%3D+%5Csigma%28g%28%5Cmathbf%7Bz%7D%2C+%5Cmathbf%7BW%7D%29%29+%3D+%5Csigma%28g%28%5Cmathbf%7BW%7D_2+%5Cdelta%28%5Cmathbf%7BW%7D_1+%5Cmathbf%7Bz%7D%29%29%29+%5Ctag2)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta) 表示ReLU激活函数， ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 表示sigmoid激活函数。 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BW%7D_1+%5Cin+%5Cmathbb%7BR%7D%5E%7B%5Cfrac%7BC%7D%7Br%7D%5Ctimes+C%7D) , ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BW%7D_2+%5Cin+%5Cmathbb%7BR%7D%5E%7BC%5Ctimes%5Cfrac%7BC%7D%7Br%7D%7D) 分别是两个全连接层的权值矩阵。 ![[公式]](https://www.zhihu.com/equation?tex=r) 则是中间层的隐层节点数，论文中指出这个值是16。

得到门控单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D) 后，最后的输出 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cmathbf%7BX%7D%7D) 表示为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BU%7D) 的向量积，即图1中的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BF%7D_%7Bscale%7D%28%5Ccdot%2C%5Ccdot%29) 操作：

![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D_c+%3D+%5Cmathbf%7BF%7D_%7Bscale%7D%28%5Cmathbf%7Bu%7D_c%2Cs_c%29+%3D+s_c+%5Ccdot+%5Cmathbf%7Bu%7D_c+%5Ctag3)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D_c) 是 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cmathbf%7BX%7D%7D) 的一个特征通道的一个Feature Map， ![[公式]](https://www.zhihu.com/equation?tex=s_c) 是门控单元 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bs%7D) （是个向量）中的一个标量值。

#### SE block理解

- SE blocks学习了每个Feature Map的动态先验；
- SE blocks可以看做在Feature Map方向的Attention，因为注意力机制的本质也是学习一组权值。

### Cifar-10数据集

CIFAR-10是一个接近普适物体的彩色图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10 个类别的RGB 彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。

每个图片的尺寸为32 × 32 ，每个类别有6000个图像，数据集中一共有50000 张训练图片和10000 张测试图片。

在官网上下载数据集并解压，得到如下文件：

<img src="C:\Users\xjy\AppData\Roaming\Typora\typora-user-images\image-20200501212845604.png" alt="image-20200501212845604" style="zoom: 50%;" />

batches.meta是元数据，包含了类别0~9对应的种类的名字（ airplane、 bird等）

data_batch_1~5都是训练数据，每一个文件中有10000组训练数据及对应的类别label

test_batch是测试数据，其中有10000组测试数据及对应的类别label

## 实验过程

### 数据集读取

由于本次实验不允许直接使用torchvision中的datasets类，因此需要自行实现数据读入。

我采取的方法是根据CIFAR-10数据集构造一个Dataset的子类Cifar10Dataset，使之能够作为`torch.utils.data.DataLoader`的参数，从而使数据集能被我们用于生成迭代数据进行训练，就像这样：

```python
train_dataset = Cifar10Dataset('./cifar-10-batches-py/', train=True)
test_dataset = Cifar10Dataset('./cifar-10-batches-py/', train=False)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)
```

Dataset的子类必须要实现两个方法：

- \_\_*getitem*\_\_(self, index)：根据index来返回数据集中标号为index的元素及其类别标签。
- \_\_*len*\_\_(self)：返回数据集的长度。

主要的读取任务我放在了*\_\_init\_\_*（）方法中，在这里把所有数据读进list或者np.array中，作为类的成员变量，即可在上述两个方法中轻松读出。

此外，为了便于后续的data augmentation操作，在\_\_init\_\_方法中加入了transform参数，用于传入torchvision.transforms对象，便于对图像进行变换操作： 

```python
    def __init__(self, root_path, train=True, transform=None):
        '''
        初始化
        :param root_path: Cifar数据集的存放目录
        :param train: 是否读取训练数据
        '''
        super(Cifar10Dataset, self).__init__()
        self.transform = transform
        self.datas = None
        self.labels = []
        # 训练文件名
        train_batches = ['data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5']
        # 测试文件名
        test_batches = ['test_batch']
        # 根据train标志选择文件
        if train:
            batches = train_batches
        else:
            batches = test_batches
        for file_name in batches:
            with open(root_path + file_name, 'rb') as file:
                dict = pickle.load(file, encoding='latin1')
                datas = dict['data'].reshape(-1, 3, 32, 32)
                labels = dict['labels']
                if self.datas is None:
                    self.datas = datas
                else:
                    self.datas = np.append(self.datas, datas, axis=0)
                self.labels += labels
        if self.transform is not None:
            self.datas = self.datas.transpose((0, 2, 3, 1)) 
        else:
            self.datas = torch.from_numpy(self.datas).type(torch.FloatTensor)   # 变为tensor张量
```

然后\_\_*getitem*\_\_(self, index)、\_\_*len*\_\_(self)方法就很容易给出：

```python
    def __getitem__(self, index):
        img = self.datas[index]
        if self.transform is not None:
            img = Image.fromarray(img)
            img = self.transform(img)
        return img, self.labels[index]

    def __len__(self):
        return len(self.datas)

```



### 网络结构实现

#### VGG

考虑到本实验采用的数据集的图像大小为32×32，因此主要基于VGG-11网络：

<img src="https://img-blog.csdnimg.cn/20190610154415242.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQ0NTM4OTg=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />

适当地改动了一些参数：

- 全连接层的节点数 4096→512
- 在每个卷积层后，加入了BN层

### 训练过程



## 实验结果与分析

### VGG参数对比分析







